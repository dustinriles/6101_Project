---
title: "Mental Health Effect on Heart Disease "
author: "Group_3 Dustin Riles, Nina Ebensperger, Pon swarnalaya Ravichandran"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    always_allow_html: true
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: '3'
    urlcolor: blue
---
#Mental Health and Heart Disease 

According to the CDC, the leading cause of death in the US is heart disease(source). Heart Disease is closely aligned to personal key indicators. Indicators like physical activity, alcohol usage, and smoking can have an impact on someone's chances of developing heart disease. There is ample research to suggest that a person's mental health can affect a person's ability to be physically active, or sleep well, or other factors that can lead to heart disease. There is also gorwing research to suggest that mental health can affect heart health by biological pathways as well as behavioral pathways. 

The question becomes:

How does the quality of someone's mental health impact their probability of heart disease and impact other contributing factors to having heart disease?

##About the Dataset

The dataset is originally from the CDC's Behavioral Risk Factor Surveillance System (BRFSS). Which conducts an annual telephone survey on the current health status of U.S. residents. The BRFSS collects the responses from over 400,000 adults living in the U.S. a year. This particular dataset is from 2020 and contains 401,958 rows and 279 columns. 

The survey consisted of questions regarding the status of the survey takers health. The questions were largely an input of yes or no. Please see breakdown of question and Variable below: 

Physical Health: 'For how many days during the past 30 days was your physical health not good?'. 

Mental health: Thinking about your mental health, for how many days during the past 30 days was your mental health not good?

Difficulty Walking: Do you have serious difficulty walking or climbing stairs? 


Kidney disease: Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease? 

Smoking: have you smoked at least 100 cigarettes in your entire life? 

The respondent would then reply with a yes or no. Other questions gave the survey takers the option to input their response.  Please see breakdown of question and Variable below: 


BMI: manual input numeric

Race: imputed value

Sleep time: On average, how many hours of sleep do you get in a 24-hour period? Numeric 


Physical Activity: Adults who reported doing physical activity or exercise during the past 30 days other than their regular job

Mental health: Thinking about your mental health, for how many days during the past 30 days was your mental health not good?

Physical health  : For how many days during the past 30 days was your physical health not good?

Lastly, variables were also in certain ranks. This was the case for the variable Age. 

Our dataset, while originally from the CDC, was taken from Kaggle where it was edited to having 18 variables deemed most relevant to heart health.

# Why this Dataset?

The Personal Key indicators of Heart Disease dataset was chosen for various reasons. 

One being that the data had already been brought down to the 18 most relevant variables. From there we were able to pick out variables for our model. 

Another reason for choosing this dataset was the sample size. With over 400,000 observations, the results can be used to make meaningful inferences about the population.  

The main factor to choosing this dataset was the wide range of methods we could use to make models. There was amble categorical values, factor values, and numeric values to use to create a model. 

Categorical Values Include 

* Heart Disease 
* Smoking
* Alcohol Drinking
* Stroke 
* Difficultly Walking
* Sex
* Diabetic 
* Asthma
* Kidney Disease
* Skin Cancer
* Race

Factor/Rank Values Include

* Age 
* General Health

Numeric Values 

* BMI
* Physical Health 
* Mental Health 
* Sleep Time
* Physical Activity 

#Data Editing

```{r init, include=F}
# Housekeeping
#rm(list = ls())


# Packages needed
#tinytex::install_tinytex()
#install.packages("dplyr")      
#install.packages("plyr")       
#install.packages("readr")   
#installed.packages('skimr', repos = "http://cran.us.r-project.org")
#install.packages
#install.packages("panelr")
#install.packages("lubridate")
#install.packages("eeptools")
#install.packages('xtable')
#install.packages('plm')
#install.packages('tseries')
#install.packages('dynlm')
#install.packages('vars')
#install.packages('broom')
#install.packages('stargazer')
#install.packages('lmtests')
#install.packages("kableExtra")
#install.packages('plotly')
#install.packages('GGally')
#install.packages('gplots')
#install.packages('ggpubr')
#install.packages('rstatix') 
library(rstatix)
library(ggpubr)
library(plotly)
library(kableExtra)
library(skimr)
library(panelr)
library(knitr)
library(stargazer)
library(dplyr)
library(readr)
library(lubridate)
library(xtable)
library(plm) 
library(corrplot)
library(tidyverse)
library(tseries) # for `adf.test()`
library(dynlm) #for function `dynlm()`
library(vars) # for function `VAR()`
library(lmtest) #for `coeftest()` and `bptest()`.
library(broom) #for `glance(`) and `tidy()`
library(ezids)
library(plot3D)
library(tinytex)
library(GGally, quietly = TRUE)
library(gplots)
library(gganimate)
#install.packages("AICcmodavg")
library(AICcmodavg)
library(randomForest)
library(dplyr)
library(MASS)
library(ezids)
library(tidyverse)
```




```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	results = "hide"
)

options(scientific=T, digits = 3) 

setwd('/Users/admin/Documents/GitHub/6101_Project/')

df = data.frame(read.csv('/Users/admin/Documents/GitHub/6101_Project/heart_2020_cleaned.csv'))

lapply(df,function(x) { length(which(is.na(x)))})
```

```{r init_data}



# Replace No for Heart Disease with 0
df$HeartDisease[df$HeartDisease == "No"] <- 0
# Replace Yes for Heart Disease with 1
df$HeartDisease[df$HeartDisease == "Yes"] <- 1
# Convert Heart Disease to factor variable
df$HeartDisease <- as.factor(df$HeartDisease)
table(df$HeartDisease)

# Replace No for Smoking with 0
df$Smoking[df$Smoking == "No"] <- 0
# Replace Yes for Smoking with 1
df$Smoking[df$Smoking == "Yes"] <- 1
# Convert Smoking to factor variable
df$Smoking <- as.factor(df$Smoking)
table(df$Smoking)

# Replace No for Alcohol Drinking with 0
df$AlcoholDrinking[df$AlcoholDrinking == "No"] <- 0
# Replace Yes for Alcohol Drinking with 1
df$AlcoholDrinking[df$AlcoholDrinking == "Yes"] <- 1
# Convert Alcohol Drinking to factor variable
df$AlcoholDrinking <- as.factor(df$AlcoholDrinking)
table(df$AlcoholDrinking)

# Replace No for Stroke with 0
df$Stroke[df$Stroke == "No"] <- 0
# Replace Yes for Stroke with 1
df$Stroke[df$Stroke == "Yes"] <- 1
# Convert Stroke to factor variable
df$Stroke <- as.factor(df$Stroke)
table(df$Stroke)

# Replace No for DiffWalking with 0
df$DiffWalking[df$DiffWalking == "No"] <- 0
# Replace Yes for DiffWalking with 1
df$DiffWalking[df$DiffWalking == "Yes"] <- 1
# Convert DiffWalking to factor variable
df$DiffWalking <- as.factor(df$DiffWalking)
table(df$DiffWalking)

# Replace Male for Sex with 0
df$Sex[df$Sex == "Male"] <- 0
# Replace Female for Sex with 1
df$Sex[df$Sex == "Female"] <- 1
# Convert Sex to factor variable
df$Sex <- as.factor(df$Sex)
table(df$Sex)

# Replace No for Physical Activity with 0
df$PhysicalActivity[df$PhysicalActivity == "No"] <- 0
# Replace Yes for Physical Activity with 1
df$PhysicalActivity[df$PhysicalActivity == "Yes"] <- 1
# Convert Physical Activity to factor variable
df$PhysicalActivity <- as.factor(df$PhysicalActivity)
table(df$PhysicalActivity)

# Replace No for Asthma with 0
df$Asthma[df$Asthma == "No"] <- 0
# Replace Yes for Asthma with 1
df$Asthma[df$Asthma == "Yes"] <- 1
# Convert Asthma to factor variable
df$Asthma <- as.factor(df$Asthma)
table(df$Asthma)

# Replace No for Kidney Disease with 0
df$KidneyDisease[df$KidneyDisease == "No"] <- 0
# Replace Yes for Kidney Disease with 1
df$KidneyDisease[df$KidneyDisease == "Yes"] <- 1
# Convert Kidney Disease to factor variable
df$KidneyDisease <- as.factor(df$KidneyDisease)
table(df$KidneyDisease)

# Replace No for Skin Cancer with 0
df$SkinCancer[df$SkinCancer == "No"] <- 0
# Repalce Yes for Skin Cancer with 1
df$SkinCancer[df$SkinCancer == "Yes"] <- 1
# Convert Skin Cancer to factor variable
df$SkinCancer <- as.factor(df$SkinCancer)
table(df$SkinCancer)

# Replace Poor for GenHealth with 0
df$GenHealth[df$GenHealth == "Poor"] <- 0
# Replace Fair for GenHealth with 1
df$GenHealth[df$GenHealth == "Fair"] <- 1
# Replace Good for GenHealth with 2
df$GenHealth[df$GenHealth == "Good"] <- 2
# Replace Very Good for GenHealth with 3
df$GenHealth[df$GenHealth == "Very good"] <- 3
# Replace Excellent for GenHealth with 4
df$GenHealth[df$GenHealth == "Excellent"] <- 4
# Convert GenHealth to factor variable
df$GenHealth <- as.factor(df$GenHealth)
table(df$GenHealth)

# Replace No for Diabetic with 0
df$Diabetic[df$Diabetic == "No"] <- 0
# Replace Yes for Diabetic with 1
df$Diabetic[df$Diabetic == "Yes"] <- 1
# Replace "No, borderline diabetes" with 2
df$Diabetic[df$Diabetic == "No, borderline diabetes"] <- 2
# Replace "Yes (during pregnancy) with 3
df$Diabetic[df$Diabetic == "Yes (during pregnancy)"] <- 3
# Convert Diabetic to factor variable
df$Diabetic <- as.factor(df$Diabetic)
table(df$Diabetic)

# Replace White for Race with 0
df$Race[df$Race == "White"] <- 0
# Replace Black for Race with 1
df$Race[df$Race == "Black"] <- 1
# Replace Hispanic for Race with 2
df$Race[df$Race == "Hispanic"] <- 2
# Replace Asian for Race with 3
df$Race[df$Race == "Asian"] <- 3
# Replace American Indian/Alaskan Native for Race with 4
df$Race[df$Race == "American Indian/Alaskan Native"] <- 4
# Replace Other for Race with 5
df$Race[df$Race == "Other"] <- 5
df$Race <- as.factor(df$Race)
table(df$Race)

df$AgeCategory[df$AgeCategory == "18-24"] <- 0
df$AgeCategory[df$AgeCategory == "25-29"] <- 1
df$AgeCategory[df$AgeCategory == "30-34"] <- 2
df$AgeCategory[df$AgeCategory == "35-39"] <- 3
df$AgeCategory[df$AgeCategory == "40-44"] <- 4
df$AgeCategory[df$AgeCategory == "45-49"] <- 5
df$AgeCategory[df$AgeCategory == "50-54"] <- 6
df$AgeCategory[df$AgeCategory == "55-59"] <- 7
df$AgeCategory[df$AgeCategory == "60-64"] <- 8
df$AgeCategory[df$AgeCategory == "65-69"] <- 9
df$AgeCategory[df$AgeCategory == "70-74"] <- 10
df$AgeCategory[df$AgeCategory == "75-79"] <- 11
df$AgeCategory[df$AgeCategory == "80 or older"] <- 12
df$AgeCategory <- as.factor(df$AgeCategory)
table(df$AgeCategory)
```
#EDA

Here we did some exploratory data analysis to get an idea of what our variables look like. 

```{r, results='markup'}





df %>%
  dplyr::select(MentalHealth, PhysicalHealth, BMI, SleepTime) %>%
  gather(variable, value) %>%
  ggplot(aes(x = value)) +
  geom_histogram(fill = "steelblue", color = "black") +
  facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
  labs(title = "Histograms of Numeric Variables", x = "Value", y = "Count") +
  theme_minimal()
  
  #bar plot count dependent variable (heart disease)
ggplot(df, aes(x = HeartDisease)) +
  geom_bar(fill = "coral") +
    theme_classic()

```

```{r}
df1 = subset(df, select = c("BMI", "PhysicalHealth", "MentalHealth", "SleepTime"))

dfcor = cor(df1)

dfcor
library(ezids)
library(tidyverse)

loadPkg("corrplot")

corrplot(dfcor, method = 'number')

```
#Confusion Matrix 
 
As we can see in the confusion matrix, the logit model, model1 does not affect the target variable (heart disease) significantly. In the confusion matrix, we can calculate that the accuracy of this model is high at 91.5%. The misclassification rate is at 8.6% which is not a high amount of errors. However the recall is at 4.8% which means of the cases that were positive only 4.8% were classified correctly. Additionally the precision of the model is at 48.97% which should be higher. 

With that analysis, we decided to move our Logit model to a random forest. 


```{r, results='markup'}

table(df$HeartDisease)
model1 <- glm(HeartDisease~MentalHealth + BMI + GenHealth + AgeCategory + Sex + PhysicalHealth + SleepTime, data = df, family = "binomial")
model2 <- glm(HeartDisease~AlcoholDrinking + BMI + PhysicalHealth + GenHealth + AgeCategory + SleepTime + Sex, data = df, family = "binomial")

models <- list (model1, model2)
model.names <- c('model1', 'model2')

summary(model1)



aictab(cand.set = models, modnames = model.names)

```

#Model Selection

Using the AICc, we tested which model would be best to select. When replacing mental health wit alcohol drinking from the model we can see that the AIC increases. So we can see that the better model to use includes mental health. Furthermore, model1 is the better model as it carries 100% of the cumulative model weight. 

Nevertheless the AIC is relatively high at 150349.

```{r, results='markup'}
#Testing the model

loadPkg("regclass")
# confusion_matrix(model1)
xkabledply( confusion_matrix(model1), title = "Confusion matrix from Logit Model" )
unloadPkg("regclass")

```
#Confusion Matrix 
 
As we can see in the confusion matrix, the logit model, model1 does not affect the target variable (heart disease) significantly. In the confusion matrix, we can calculate that the accuracy of this model is high at 91.5%. The misclassification rate is at 8.6% which is not a high amount of errors. 

```{r, results='markup'}
set.seed(2904)
s<-sort(sample(nrow(df),nrow(df)*.8))
train<-df[s,]
test<-df[-s,]
rm(s)


rf2<-randomForest(HeartDisease~MentalHealth + BMI + GenHealth + AgeCategory + Sex + PhysicalHealth + SleepTime,ntree=200,data=train)
rf2




set.seed(2904)
rf<-randomForest(HeartDisease~.,ntree=200,data=train)
rf
plot(rf$err.rate[,1],type="l",main="Random Forest Error Rate",xlab="Number of Trees")
varImpPlot(rf,main="Variable Importance Plot for Random Forest")
rfpred<-predict(rf,test,type="class")
rft<-table(test$HeartDisease,rfpred)
rft

```
# Random Forests 

Since our data is highly unbalanced, we decided a random forest would be best to use as it is more powerful in its accuracy than a standard decision tree. 

I choose to make a random forest that contains 200 trees, and while each tree may be over-fitted on its own. The final result considering all the trees together, will be as prone to the over fitting. The first step I did was split the dataset into train and test where 80% of the data will be used for training and the other 20% will be used for testing. 

We can see that we used every 4 variables at every split, coming from the square-root of 18. Which increases the randomness of our algorithm. 

After running the train data in the random forest model using the same variables as our logit model, we find that the results are still not ideal. We find that the accuracy rate for the random forest is at 91.5%. The recall rate is at 1.7%, so when the case is actually yes, the model only predicts yes 1.7% of the time. The precision is at 49.7% which is higher than the previous model but could be better. Additionally, the misclassification rate is at a 8.59%. So the random forest not making significantly less mistakes. Lastly we can see that our false negative rate is at 98.32%, meaning that 98.32% of the cases that were positive were falsely classified as being negatives. This is an extremely high error rate and we will have to make another model.  


Therefore, we decieded to use all variables to improve the precision of the model. In comparison, the second random forest had  precision rate of 51.8%.  With an accuracy rate of 91.5% and a recall of 8.81%. Furthermore, our false negative rate is at 0.92, meaning that 92% of the cases that were positive were falsely classified as negatives. This is still fairly high, but we can see it is lower than our previous models. The misclassification rate at 8.53%, so it is making the same amount of mistakes as the previous random forest. Nevertheless, we can conclude that this model is the better model to use to explain our target variable. 

Supplemental to the random forest, we included two graphs. One to show that the error rate of the forest levels out at 100 trees and is fairly low. The other to demonstrate the most important variables. As you can see, the most influential variables are BMI, Age, and Sleep time. We did see that sleep time and mental health have a small relationship in the correlation plot. 

# References 
CDC. (2022, September 6). Leading Causes of Death. National Center for Health Statistics. https://www.cdc.gov/nchs/fastats/leading-causes-of-death.htm

Heart Disease and Mental Health Disorders. (2020, May 6). CDC. https://www.cdc.gov/heartdisease/mentalhealth.htm
